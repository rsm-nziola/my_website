[
  {
    "objectID": "blog/hw1/hw1_questions.html",
    "href": "blog/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment found that offering a matching donation significantly increased both the likelihood that individuals would donate and the average amount of each donation, Specifically, simply mentioning a match increased the probability of donating by about 22% and raised revenue per letter by 19%. However, increasing the offer from a 1:1 ratio to doubling or tripling the amount did not lead to a statistically significant increase in donations. This finding contradicts the assumption that larger matches are more motivating.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#introduction",
    "href": "blog/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment found that offering a matching donation significantly increased both the likelihood that individuals would donate and the average amount of each donation, Specifically, simply mentioning a match increased the probability of donating by about 22% and raised revenue per letter by 19%. However, increasing the offer from a 1:1 ratio to doubling or tripling the amount did not lead to a statistically significant increase in donations. This finding contradicts the assumption that larger matches are more motivating.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#data",
    "href": "blog/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\nshow code\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nshow code\nimport statsmodels.api as sm\n\ndef summarize_ols(model):\n    coef = model.params\n    se = model.bse\n    tstat = model.tvalues\n    pval = model.pvalues\n    conf_int = model.conf_int()\n\n    print(\"📊 OLS Regression Summary\")\n    print(\"-\" * 40)\n    for var in coef.index:\n        print(f\"Variable: {var}\")\n        print(f\"  Coefficient     : {coef[var]:.4f}\")\n        print(f\"  Std. Error      : {se[var]:.4f}\")\n        print(f\"  t-statistic     : {tstat[var]:.4f}\")\n        print(f\"  p-value         : {pval[var]:.4f}\")\n        print(f\"  95% CI          : [{conf_int.loc[var, 0]:.4f}, {conf_int.loc[var, 1]:.4f}]\")\n        print(\"-\" * 40)\n\nimport statsmodels.formula.api as smf\n\ndef summarize_probit(model):\n    coef = model.params\n    se = model.bse\n    zstat = model.tvalues\n    pval = model.pvalues\n    conf_int = model.conf_int()\n\n    print(\"📈 Probit Regression Summary\")\n    print(\"-\" * 40)\n    for var in coef.index:\n        print(f\"Variable: {var}\")\n        print(f\"  Coefficient     : {coef[var]:.4f}\")\n        print(f\"  Std. Error      : {se[var]:.4f}\")\n        print(f\"  z-statistic     : {zstat[var]:.4f}\")\n        print(f\"  p-value         : {pval[var]:.4f}\")\n        print(f\"  95% CI          : [{conf_int.loc[var, 0]:.4f}, {conf_int.loc[var, 1]:.4f}]\")\n        print(\"-\" * 40)\n\nvariables = ['mrm2', 'hpa', 'freq', 'female'] \n\nresults = []\n\ndef t_test_formula(x_treat, x_control):\n    # T-test using pooled standard error\n    mean1, mean2 = np.mean(x_treat), np.mean(x_control)\n    std1, std2 = np.std(x_treat, ddof=1), np.std(x_control, ddof=1)\n    n1, n2 = len(x_treat), len(x_control)\n    se = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n    t_stat = (mean1 - mean2) / se\n    return t_stat, mean1 - mean2\n\nfor var in variables:\n    treat_group = df[df['treatment'] == 1][var].dropna()\n    control_group = df[df['treatment'] == 0][var].dropna()\n\n    # --- T-test\n    t_stat, t_diff = t_test_formula(treat_group, control_group)\n\n    # --- Linear regression\n    X = sm.add_constant(df['treatment'])\n    y = df[var]\n    model = sm.OLS(y, X).fit()\n    reg_coef = model.params['treatment']\n    reg_se = model.bse['treatment']\n    reg_tstat = model.tvalues['treatment']\n\n    results.append({\n        'Variable': var,\n        'T-test statistic': round(t_stat, 4),\n        'Mean Diff (T-test)': round(t_diff, 4),\n        'Regression Coef': round(reg_coef, 4),\n        'T-stat (Regression)': round(reg_tstat, 4),\n        'Match?': 'Yes' if np.isclose(t_stat, reg_tstat, atol=1e-4) else 'No'\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n\n\n\n\nVariable\nT-test statistic\nMean Diff (T-test)\nRegression Coef\nT-stat (Regression)\nMatch?\n\n\n\n\n0\nmrm2\n0.1195\n0.0137\nNaN\nNaN\nNo\n\n\n1\nhpa\n0.9704\n0.6371\n0.6371\n0.9441\nNo\n\n\n2\nfreq\n-0.1108\n-0.0120\n-0.0120\n-0.1109\nYes\n\n\n3\nfemale\n-1.7535\n-0.0075\nNaN\nNaN\nNo\n\n\n\n\n\n\n\nThis process mimics Table 1 in the Karlan & List paper, which reports summary statistics by treatment group to demonstrate that the random assignment worked. If the treatment and control groups look similar before the treatment, then any differences after treatment can more confidently be attributed to the treatment itself — and not to pre-existing differences.\nThe analysis shows that the groups were well-balanced on these pre-treatment variables. No statistically significant differences were found, which supports the validity of the experiment. Table 1 is included in the paper for exactly this reason: to demonstrate that randomization created comparable groups and to reassure readers that the treatment effects are credible."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#experimental-results",
    "href": "blog/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. The donation rates in the treatment vs. control groups are 2.2% and 1.79%, respectively.\n\n\nshow code\nproportion_treatment = df[df['treatment'] == 1]['gave'].mean()\nproportion_control = df[df['treatment'] == 0]['gave'].mean()\n\n# Barplot setup\nlabels = ['Control', 'Treatment']\nvalues = [proportion_control, proportion_treatment]\n\nplt.figure(figsize=(6, 4))\nplt.bar(labels, values)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rates by Group')\nplt.ylim(0, max(values)*1.2)\n\n# Add percentage labels above bars\nfor i, v in enumerate(values):\n    plt.text(i, v + 0.0005, f\"{v:.2%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNext, I utilize t-tests and linear regression to analyze if the difference in donation rate proportions are statistically significant.\n\n\nshow code\n# --- T-TEST: Difference in proportions for binary outcome 'gave' between treatment and control ---\n\n# Separate treatment and control groups\ngave_treatment = df[df['treatment'] == 1]['gave']\ngave_control = df[df['treatment'] == 0]['gave']\n\n# Calculate group means and sizes\np1 = gave_treatment.mean()\np2 = gave_control.mean()\nn1 = gave_treatment.shape[0]\nn2 = gave_control.shape[0]\n\n# Calculate standard error and t-statistic\nse = np.sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n2))\nt_stat = (p1 - p2) / se\n\n# Calculate degrees of freedom and p-value\ndf_ttest = n1 + n2 - 2\np_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=df_ttest))\n\nprint(\"T-test results:\")\nprint(f\"  Treatment mean: {p1:.4f}\")\nprint(f\"  Control mean: {p2:.4f}\")\nprint(f\"  T-statistic: {t_stat:.4f}\")\nprint(f\"  P-value: {p_value:.4f}\\n\")\n\n\n# --- LINEAR REGRESSION: gave ~ treatment ---\n\n# Run regression\nX = sm.add_constant(df['treatment'])\ny = df['gave']\nmodel = sm.OLS(y, X).fit()\n\nsummarize_ols(model)\n\n\nT-test results:\n  Treatment mean: 0.0220\n  Control mean: 0.0179\n  T-statistic: 3.2095\n  P-value: 0.0013\n\n📊 OLS Regression Summary\n----------------------------------------\nVariable: const\n  Coefficient     : 0.0179\n  Std. Error      : 0.0011\n  t-statistic     : 16.2246\n  p-value         : 0.0000\n  95% CI          : [0.0157, 0.0200]\n----------------------------------------\nVariable: treatment\n  Coefficient     : 0.0042\n  Std. Error      : 0.0013\n  t-statistic     : 3.1014\n  p-value         : 0.0019\n  95% CI          : [0.0015, 0.0068]\n----------------------------------------\n\n\nThe p-value is significant at the 95% confidence level, allowing us to conclude that the difference in donation rates between treatment and control is statistically significant and not due to random chance alone.\nWhen people were told their donation would be matched, they were more likely to give compared to those who did not receive the same offer. Even though the difference in donation rates is not huge, it is meaningful enough to show that the match offer was impactful on human behavior.\nWe also ran a regression that supported the same conclusion: the offer of a match made people slightly more likely to donate.\nIn short, match-based donation programs achieve slightly higher dconation rates than non-matched programs, suggesting that the extra push from donation matching is effective in boosting donation rates.\n\n\nshow code\nimport statsmodels.formula.api as smf\n\n# Probit regression of 'gave' on 'treatment'\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nprint(summarize_probit(probit_model))\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n📈 Probit Regression Summary\n----------------------------------------\nVariable: Intercept\n  Coefficient     : -2.1001\n  Std. Error      : 0.0233\n  z-statistic     : -90.0728\n  p-value         : 0.0000\n  95% CI          : [-2.1458, -2.0544]\n----------------------------------------\nVariable: treatment\n  Coefficient     : 0.0868\n  Std. Error      : 0.0279\n  z-statistic     : 3.1129\n  p-value         : 0.0019\n  95% CI          : [0.0321, 0.1414]\n----------------------------------------\nNone\n\n\nIn further support of the t-test and regression conclusion, I used probit regression, which confirms our initial findings with a statistically significant and positive coefficient value of 0.0868 on treatment. This suggests that holding all else constant, participants are more likely to donate if they are part of the treatment group rather than the control.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nshow code\nfrom scipy.stats import ttest_ind\n\n# Subset groups based on match ratio\ngave_ratio1 = df[df['ratio'] == 1]['gave']\ngave_ratio2 = df[df['ratio'] == 2]['gave']\ngave_ratio3 = df[df['ratio'] == 3]['gave']\n\n# Perform independent t-tests\nt_2v1, p_2v1 = ttest_ind(gave_ratio2, gave_ratio1, equal_var=False)\nt_3v2, p_3v2 = ttest_ind(gave_ratio3, gave_ratio2, equal_var=False)\nt_3v1, p_3v1 = ttest_ind(gave_ratio3, gave_ratio1, equal_var=False)\n\n# Print results\nprint(\"T-Test Results (Donation Likelihood by Match Ratio):\\n\")\nprint(f\"2:1 vs 1:1 -&gt; t = {t_2v1:.3f}, p = {p_2v1:.4f}\")\nprint(f\"3:1 vs 2:1 -&gt; t = {t_3v2:.3f}, p = {p_3v2:.4f}\")\nprint(f\"3:1 vs 1:1 -&gt; t = {t_3v1:.3f}, p = {p_3v1:.4f}\")\n\n\nT-Test Results (Donation Likelihood by Match Ratio):\n\n2:1 vs 1:1 -&gt; t = 0.965, p = 0.3345\n3:1 vs 2:1 -&gt; t = 0.050, p = 0.9600\n3:1 vs 1:1 -&gt; t = 1.015, p = 0.3101\n\n\nThe t-test series analyzing the size of the match ratio suggest that there is no significant differences in donation likelihood by match ratio. This supports the authors’ findings that while the essence of a match offer does increase donation likelihood, the actual size of the match has no effect on increasing donation rates.\n\n\nshow code\n# Regress using 'ratio' as a categorical variable (automatically handles dummies)\ncat_model = smf.ols('gave ~ C(ratio)', data=df).fit()\n\nprint(\"\\nRegression with Categorical Variable:\")\nprint(summarize_ols(cat_model))\n\n\n\nRegression with Categorical Variable:\n📊 OLS Regression Summary\n----------------------------------------\nVariable: Intercept\n  Coefficient     : 0.0179\n  Std. Error      : 0.0011\n  t-statistic     : 16.2245\n  p-value         : 0.0000\n  95% CI          : [0.0157, 0.0200]\n----------------------------------------\nVariable: C(ratio)[T.1]\n  Coefficient     : 0.0029\n  Std. Error      : 0.0017\n  t-statistic     : 1.6615\n  p-value         : 0.0966\n  95% CI          : [-0.0005, 0.0063]\n----------------------------------------\nVariable: C(ratio)[T.2]\n  Coefficient     : 0.0048\n  Std. Error      : 0.0017\n  t-statistic     : 2.7445\n  p-value         : 0.0061\n  95% CI          : [0.0014, 0.0082]\n----------------------------------------\nVariable: C(ratio)[T.3]\n  Coefficient     : 0.0049\n  Std. Error      : 0.0017\n  t-statistic     : 2.8016\n  p-value         : 0.0051\n  95% CI          : [0.0015, 0.0083]\n----------------------------------------\nNone\n\n\nNext, I will use regression with ratio as a categorical variable in order to confirm the t-test results.\nIntercept (0.0179): This is the average donation rate for the control group - 1.79% of people donated in that group.\nC(ratio)[T.1] = 0.0029: People who were offered a 1:1 match were slightly more likely to donate than those in the control group, but the result isn’t statistically significant (p = 0.097). This suggests a possible increase, but not significantly.\nC(ratio)[T.2] = 0.0048: People who were offered a 2:1 match were more likely to donate compared to the control group. This result is statistically significant (p = 0.006), suggesting that the 2:1 offer had a real impact, but at less than half a percentage point.\nC(ratio)[T.3] = 0.0049: Similarly, those offered a 3:1 match were also more likely to give than people in the control group. This effect is statistically significant (p = 0.005) and about the same size as the 2:1 effect at just below half a percentage point.\nThe regression output suggests that the differences between control, 1:1 match, 2:1 match, and 3:1 match are very small, telling us that the difference between match amounts does not impact donation probaility.\n\n\nshow code\nimport statsmodels.formula.api as smf\n\n# --- RAW RESPONSE RATE DIFFERENCES ---\n# Compute average donation rates by ratio level\nresponse_rate_1to1 = df[df['ratio'] == 1]['gave'].mean()\nresponse_rate_2to1 = df[df['ratio'] == 2]['gave'].mean()\nresponse_rate_3to1 = df[df['ratio'] == 3]['gave'].mean()\n\n# Calculate raw differences\nraw_diff_2v1 = response_rate_2to1 - response_rate_1to1\nraw_diff_3v2 = response_rate_3to1 - response_rate_2to1\n\nprint(\"Raw response rate differences:\")\nprint(f\"  2:1 vs 1:1: {raw_diff_2v1:.4f}\")\nprint(f\"  3:1 vs 2:1: {raw_diff_3v2:.4f}\\n\")\n\n# --- REGRESSION APPROACH ---\n# Create dummy variables if not already present\ndf['ratio2'] = (df['ratio'] == 2).astype(int)\ndf['ratio3'] = (df['ratio'] == 3).astype(int)\n\n# Fit model using 1:1 as baseline\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=df).fit()\n\n# Extract regression-based differences\nreg_diff_2v1 = model.params['ratio2']\nreg_diff_3v2 = model.params['ratio3'] - model.params['ratio2']\n\nprint(\"Regression-estimated response rate differences:\")\nprint(f\"  2:1 vs 1:1: {reg_diff_2v1:.4f}\")\nprint(f\"  3:1 vs 2:1: {reg_diff_3v2:.4f}\")\n\n\nRaw response rate differences:\n  2:1 vs 1:1: 0.0019\n  3:1 vs 2:1: 0.0001\n\nRegression-estimated response rate differences:\n  2:1 vs 1:1: 0.0036\n  3:1 vs 2:1: 0.0001\n\n\nI conclude that the different sizes of matched donations do not effectively change response rates, evident from the very small difference in donation proportion between match groups regardless of the match ratio.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nshow code\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# --- T-TEST ---\n# Compare donation amounts between treatment and control groups\namount_treatment = df[df['treatment'] == 1]['amount']\namount_control = df[df['treatment'] == 0]['amount']\n\n# T-test (unequal variance)\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\nprint(\"T-test: Donation Amount ~ Treatment\")\nprint(f\"  T-statistic: {t_stat:.3f}\")\nprint(f\"  P-value: {p_val:.4f}\\n\")\n\n# --- REGRESSION ---\n# Run bivariate regression\nmodel = smf.ols('amount ~ treatment', data=df).fit()\n\nprint(\"Bivariate Linear Regression: Donation Amount ~ Treatment\")\nprint(summarize_ols(model))\n\n\nT-test: Donation Amount ~ Treatment\n  T-statistic: 1.918\n  P-value: 0.0551\n\nBivariate Linear Regression: Donation Amount ~ Treatment\n📊 OLS Regression Summary\n----------------------------------------\nVariable: Intercept\n  Coefficient     : 0.8133\n  Std. Error      : 0.0674\n  t-statistic     : 12.0630\n  p-value         : 0.0000\n  95% CI          : [0.6811, 0.9454]\n----------------------------------------\nVariable: treatment\n  Coefficient     : 0.1536\n  Std. Error      : 0.0826\n  t-statistic     : 1.8605\n  p-value         : 0.0628\n  95% CI          : [-0.0082, 0.3154]\n----------------------------------------\nNone\n\n\nThis analysis tells us whether offering a matching ratio of any amount influences the dollar amount of the donation, regardless of whether or not a donation was made (meaning zeros are included for non-donors).\nIntercept (0.8133): This is the average donation amount in the control group — around $0.81 per person (including all the zeroes from people who didn’t donate).\nTreatment coefficient (0.1536): This tells us that people in the treatment group gave about 15 cents more, on average, than those in the control group.\nThe difference is marginally signficant with a p-value of 0.06, but the difference between the two values is relatively small, and we cannot definitively attribute the increase to the treatment effect.\n\n\nshow code\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# --- FILTER: Only people who donated ---\ndf_donors = df[df['gave'] == 1]\n\n# --- T-TEST on Amount Given ---\namount_treatment = df_donors[df_donors['treatment'] == 1]['amount']\namount_control = df_donors[df_donors['treatment'] == 0]['amount']\n\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\nprint(\"T-test (Amount Given | Gave = 1):\")\nprint(f\"  T-statistic: {t_stat:.3f}\")\nprint(f\"  P-value: {p_val:.4f}\\n\")\n\n# --- REGRESSION: Donation Amount ~ Treatment (only donors) ---\nmodel = smf.ols('amount ~ treatment', data=df_donors).fit()\n\nprint(\"OLS Regression (Amount Given | Gave = 1):\")\nprint(summarize_ols(model))\n\n\nT-test (Amount Given | Gave = 1):\n  T-statistic: -0.585\n  P-value: 0.5590\n\nOLS Regression (Amount Given | Gave = 1):\n📊 OLS Regression Summary\n----------------------------------------\nVariable: Intercept\n  Coefficient     : 45.5403\n  Std. Error      : 2.4234\n  t-statistic     : 18.7921\n  p-value         : 0.0000\n  95% CI          : [40.7850, 50.2956]\n----------------------------------------\nVariable: treatment\n  Coefficient     : -1.6684\n  Std. Error      : 2.8724\n  t-statistic     : -0.5808\n  p-value         : 0.5615\n  95% CI          : [-7.3048, 3.9680]\n----------------------------------------\nNone\n\n\nThis regression answers the question: “Among people who chose to donate, did those who received a match offer give more or less money than those who didn’t?”\nIntercept (45.54): This is the average amount donated by people in the control group, i.e., those who donated without being offered a match. On average, they gave about $45.54.\nTreatment coefficient (-1.6684): This tells us that people in the treatment group gave about $1.67 less than those in the control group (among those who donated). However, the p-value is 0.561, which is very high, meaning this difference is not statistically significant. In other words, there’s no strong evidence that the treatment influenced how much donors gave.\nThis analysis suggests that offering a match did not affect how much donors gave, once they decided to give. While people in the treatment group gave a little less on average, this could easily be due to random chance. The treatment seemed to influence whether people donated, but not how much they gave once they did.\nWe can interpret causality here since the treatment was randomly assigned. However, the causality is conditional on the fact that the participant chose to donate, not generalized to the entire population.\n\n\nshow code\nimport matplotlib.pyplot as plt\n\n# Filter to only people who donated\ndf_donors = df[df['gave'] == 1]\n\n# Split into treatment and control\ntreatment_donors = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_donors = df_donors[df_donors['treatment'] == 0]['amount']\n\n# Calculate means\nmean_treatment = treatment_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_control:.2f}')\naxes[0].set_title(\"Control Group (Donors Only)\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_treatment:.2f}')\naxes[1].set_title(\"Treatment Group (Donors Only)\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plots above show the distribution of donation amounts among people who donated, separated by treatment group.\nThe left panel displays the control group (those who did not receive a match offer). Their donation amounts are spread out, with most donors giving smaller amounts, and a few giving significantly more. The average donation in this group is about $45.54, marked by the red dashed line.\nThe right panel shows the treatment group — those who were offered a matching donation. This group also gave mostly small amounts, with a few larger gifts, and more donations were made. The average donation here is slightly lower, at about $43.87, also marked with a red dashed line."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#simulation-experiment",
    "href": "blog/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nshow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter to only people who donated\ndf_donors = df[df['gave'] == 1]\n\n# Control and treatment donation amounts\ncontrol = df_donors[df_donors['treatment'] == 0]['amount'].values\ntreatment = df_donors[df_donors['treatment'] == 1]['amount'].values\n\n# True difference in means\ntrue_diff = treatment.mean() - control.mean()\n\n# Simulate draws\nnp.random.seed(42)\ncontrol_draws = np.random.choice(control, size=100000, replace=True)\ntreatment_draws = np.random.choice(treatment, size=10000, replace=True)\n\n# Sample 10,000 control values from the 100,000 to match length\ncontrol_draws_subset = np.random.choice(control_draws, size=10000, replace=False)\n\n# Calculate vector of differences\ndifferences = treatment_draws - control_draws_subset\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.2f}')\nplt.title(\"Cumulative Average of Simulated Differences in Donation Amounts\")\nplt.xlabel(\"Number of Simulated Differences\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe cumulative average clearly approaches the true difference in means.\nIn the plot, you can see that although the average difference between treatment and control donations fluctuates quite a bit at the beginning (as expected with small sample sizes), those fluctuations start to smooth out. As more simulated differences are added — getting closer to 10,000 — the blue line gradually stabilizes around the red dashed line, which marks the true difference in means (approximately –1.67).\nThis behavior demonstrates the law of large numbers in action: as the number of observations increases, the cumulative average becomes more consistent and converges toward the actual value we’re trying to estimate.\n\n\nCentral Limit Theorem\n\n\nshow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Simulation Settings ---\nnp.random.seed(42)\np_control = 0.018  # Probability of success in control group\np_treatment = 0.022  # Probability of success in treatment group\nsample_sizes = [50, 200, 500, 1000]  # Different sample sizes to simulate\nn_simulations = 1000  # Number of simulations per sample size\ntrue_diff = p_treatment - p_control  # True difference in proportions\n\n# --- Function to simulate sampling distribution of mean differences ---\ndef simulate_diffs(n, p_control, p_treatment, reps=1000):\n    diffs = []\n    for _ in range(reps):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = np.mean(treatment_sample) - np.mean(control_sample)\n        diffs.append(diff)\n    return np.array(diffs)\n\n# --- Generate and Plot Histograms ---\nfig, axes = plt.subplots(2, 2, figsize=(12, 10), sharey=True)\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    simulated_diffs = simulate_diffs(n, p_control, p_treatment, n_simulations)\n    \n    ax = axes[i]\n    ax.hist(simulated_diffs, bins=30, density=True, alpha=0.7, edgecolor='black', color='skyblue')\n    ax.axvline(0, color='black', linestyle='--', linewidth=1, label='Zero')\n    ax.axvline(true_diff, color='red', linestyle='--', linewidth=2, label=f'True Diff = {true_diff:.3f}')\n    \n    ax.set_title(f\"Sample Size = {n}\")\n    ax.set_xlabel(\"Difference in Means\")\n    ax.set_ylabel(\"Density\")\n    ax.legend()\n\nplt.suptitle(\"Sampling Distributions of Differences in Proportions\", fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\nAs the sample size increases, zero shifts from being in the middle of the distribution to being in the tail.\nAt smaller sample sizes (like n = 50), the distribution of average differences is wide and scattered. Zero sits near the center, meaning there’s a lot of uncertainty — the observed differences could easily include zero, suggesting no clear effect.\nAs the sample size grows (to 200 and 500), the distribution starts to tighten and center around a slightly negative value. By n = 500, zero is already drifting out of the middle, suggesting that most random samples are detecting a small negative difference between treatment and control.\nAt the largest sample size (n = 1000), the distribution is sharply peaked and centered well below zero. Here, zero clearly lies in the tail, which tells us that the chance of seeing no difference (zero) is low if the treatment truly causes lower donation amounts.\nZero is in the middle when sample sizes are small, meaning we can’t distinguish treatment from control. Zero moves to the tail as sample size grows, giving us stronger evidence that the treatment had a real (though small) effect."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html",
    "href": "blog/hw3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/hw3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n\nbrand = [\"N\", \"P\", \"H\"] \nad = [\"Yes\", \"No\"]\nprice = np.arange(8, 33, 4)\n\nimport itertools\nprofiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=[\"brand\", \"ad\", \"price\"])\nm = len(profiles)\n\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\ndef sim_one(id):\n    datlist = []\n\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled.insert(0, \"resp\", id)\n        sampled.insert(1, \"task\", t)\n\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util) +\n            sampled[\"ad\"].map(a_util) +\n            sampled[\"price\"].apply(p_util)\n        ).round(10)\n\n        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        datlist.append(sampled)\n\n    return pd.concat(datlist, ignore_index=True)\n\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\nprint(conjoint_data.head())\n\n   resp  task brand  ad  price  choice\n0     1     1     P  No     32       0\n1     1     1     N  No     28       0\n2     1     1     N  No     24       1\n3     1     2     H  No     28       0\n4     1     2     H  No      8       1\n\n\n\n3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\n\ndf = pd.read_csv('conjoint_data.csv')\n\nimport pandas as pd\n\nconjoint_clean = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\nconjoint_clean = conjoint_clean.rename(columns={\n    \"brand_N\": \"brand_netflix\",\n    \"brand_P\": \"brand_prime\",\n    \"ad_Yes\": \"ad_yes\"\n})\n\nconjoint_clean[\"alt_id\"] = conjoint_clean.groupby([\"resp\", \"task\"]).cumcount() + 1\n\nconjoint_clean = conjoint_clean[[\n    \"resp\", \"task\", \"alt_id\",\n    \"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\",\n    \"choice\"\n]]\n\n\nprint(conjoint_clean.head(10))\n\n   resp  task  alt_id  brand_netflix  brand_prime  ad_yes  price  choice\n0     1     1       1           True        False    True     28       1\n1     1     1       2          False        False    True     16       0\n2     1     1       3          False         True    True     16       0\n3     1     2       1           True        False    True     32       0\n4     1     2       2          False         True    True     16       1\n5     1     2       3           True        False    True     24       0\n6     1     3       1          False         True   False      8       0\n7     1     3       2          False        False    True     24       1\n8     1     3       3           True        False   False     24       0\n9     1     4       1          False         True   False     28       0\n\n\n\n\n4. Estimation via Maximum Likelihood\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\n\nX = conjoint_clean[[\"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\"]].astype(float).values\ny = conjoint_clean[\"choice\"].values\n\ngroups = conjoint_clean.groupby([\"resp\", \"task\"]).indices\nchoice_sets = list(groups.values())\n\ndef log_likelihood(beta):\n    log_lik = 0\n    for idx in choice_sets:\n        X_set = X[idx]\n        y_set = y[idx]\n        utilities = X_set @ beta\n        exp_utilities = np.exp(utilities)\n        probs = exp_utilities / np.sum(exp_utilities)\n        log_lik += np.log(probs[y_set == 1][0])\n    return -log_lik  \n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(log_likelihood, initial_beta, method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nz = 1.96  # critical value for 95% CI\nconf_ints = np.vstack([beta_hat - z * std_errors, beta_hat + z * std_errors]).T\n\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nsummary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": std_errors,\n    \"95% CI Lower\": conf_ints[:, 0],\n    \"95% CI Upper\": conf_ints[:, 1]\n})\n\nprint(summary)\n\n      Parameter  Estimate  Std. Error  95% CI Lower  95% CI Upper\n0  beta_netflix  0.941195    0.100639      0.743942      1.138448\n1    beta_prime  0.501616    0.135631      0.235778      0.767453\n2      beta_ads -0.731994    0.089276     -0.906975     -0.557014\n3    beta_price -0.099480    0.006330     -0.111887     -0.087074\n\n\n\n\n5. Estimation via Bayesian Methods\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\n\nX = conjoint_clean[[\"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\"]].astype(float).values\ny = conjoint_clean[\"choice\"].values\ngroup_keys = conjoint_clean[[\"resp\", \"task\"]].apply(tuple, axis=1)\nchoice_sets = [np.where(group_keys == key)[0] for key in sorted(set(group_keys))]\n\ndef log_likelihood(beta):\n    log_lik = 0.0\n    for idx in choice_sets:\n        X_set = X[idx]\n        y_set = y[idx]\n        utilities = X_set @ beta\n        log_lik += utilities[y_set == 1][0] - logsumexp(utilities)\n    return log_lik\n\n\ndef log_prior(beta):\n    prior_var = np.array([25, 25, 25, 1])  \n    return -0.5 * np.sum((beta ** 2) / prior_var)\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\nn_draws = 11000\nburn_in = 1000\nn_params = 4\nsamples = np.zeros((n_draws, n_params))\nbeta_current = np.zeros(n_params)\nlog_post_current = log_posterior(beta_current)\n\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\nfor t in range(n_draws):\n    proposal = beta_current + np.random.normal(loc=0, scale=proposal_sd)\n    log_post_proposal = log_posterior(proposal)\n    \n    log_accept_ratio = log_post_proposal - log_post_current\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    \n    samples[t] = beta_current\n\nsamples_post = samples[burn_in:]\nposterior_means = np.mean(samples_post, axis=0)\nposterior_std = np.std(samples_post, axis=0)\nposterior_ci_lower = np.percentile(samples_post, 2.5, axis=0)\nposterior_ci_upper = np.percentile(samples_post, 97.5, axis=0)\n\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nposterior_summary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Posterior Mean\": posterior_means,\n    \"Std. Dev\": posterior_std,\n    \"95% CI Lower\": posterior_ci_lower,\n    \"95% CI Upper\": posterior_ci_upper\n})\n\nprint(posterior_summary)\n\n      Parameter  Posterior Mean  Std. Dev  95% CI Lower  95% CI Upper\n0  beta_netflix        0.930949  0.108311      0.725842      1.157317\n1    beta_prime        0.488224  0.106468      0.280797      0.702842\n2      beta_ads       -0.723895  0.091051     -0.898098     -0.538874\n3    beta_price       -0.099514  0.006269     -0.112154     -0.087608\n\n\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\n\nimport matplotlib.pyplot as plt\n\nposterior_samples = samples_post  \n\nbeta_idx = 0 \nparam_name = \"Beta_Netflix\"\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(posterior_samples[:, beta_idx], color=\"blue\", alpha=0.6)\nplt.title(f\"Trace Plot: {param_name}\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\n\nplt.subplot(1, 2, 2)\nplt.hist(posterior_samples[:, beta_idx], bins=30, color=\"skyblue\", edgecolor=\"black\")\nplt.title(f\"Posterior Distribution: {param_name}\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe results from the Maximum Likelihood approach are approximately the same compared to the estimation via Bayesian Methods.\n\n\n6. Discussion\nIf we did not simulate the data and only saw the parameter estimates, we would interpret the values as reflecting consumer preferences inferred from real-world consumer trends. \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) suggests that customers tend to prefer Netflix over Amazon Prime, which tracks with our intuition about streaming service brands. The higher beta parameter means that customers are more likely to choose Netflix over Prime, indicating Netflix’s higher percieved value or brand equity.\nIt makes sense that \\(\\beta_\\text{price}\\) is negative because our intuition of supply and demand is that as prices increase, utility decrease. All else equal, consumers prefer lower costs.\nFor “real world” conjoint data, we need to allow differences by individual. Therefore, we would need to allow the beta parameters to vary by individual, instead of having a fixed value across the population. Thus, the simulated data would need to have individual-level coefficients,generate choices based off of the individual beta parameters, and then estimate with a Bayesian hierarchical model.\nThis approach captures heterogeneity in preferences, which is a key aspect of real-world conjoint data, allowing different people to value different choices differently."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\n\ndf = pd.read_csv('conjoint_data.csv')\n\nimport pandas as pd\n\nconjoint_clean = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\nconjoint_clean = conjoint_clean.rename(columns={\n    \"brand_N\": \"brand_netflix\",\n    \"brand_P\": \"brand_prime\",\n    \"ad_Yes\": \"ad_yes\"\n})\n\nconjoint_clean[\"alt_id\"] = conjoint_clean.groupby([\"resp\", \"task\"]).cumcount() + 1\n\nconjoint_clean = conjoint_clean[[\n    \"resp\", \"task\", \"alt_id\",\n    \"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\",\n    \"choice\"\n]]\n\n\nprint(conjoint_clean.head(10))\n\n   resp  task  alt_id  brand_netflix  brand_prime  ad_yes  price  choice\n0     1     1       1           True        False    True     28       1\n1     1     1       2          False        False    True     16       0\n2     1     1       3          False         True    True     16       0\n3     1     2       1           True        False    True     32       0\n4     1     2       2          False         True    True     16       1\n5     1     2       3           True        False    True     24       0\n6     1     3       1          False         True   False      8       0\n7     1     3       2          False        False    True     24       1\n8     1     3       3           True        False   False     24       0\n9     1     4       1          False         True   False     28       0"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\n\nX = conjoint_clean[[\"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\"]].astype(float).values\ny = conjoint_clean[\"choice\"].values\n\ngroups = conjoint_clean.groupby([\"resp\", \"task\"]).indices\nchoice_sets = list(groups.values())\n\ndef log_likelihood(beta):\n    log_lik = 0\n    for idx in choice_sets:\n        X_set = X[idx]\n        y_set = y[idx]\n        utilities = X_set @ beta\n        exp_utilities = np.exp(utilities)\n        probs = exp_utilities / np.sum(exp_utilities)\n        log_lik += np.log(probs[y_set == 1][0])\n    return -log_lik  \n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(log_likelihood, initial_beta, method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nz = 1.96  # critical value for 95% CI\nconf_ints = np.vstack([beta_hat - z * std_errors, beta_hat + z * std_errors]).T\n\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nsummary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": std_errors,\n    \"95% CI Lower\": conf_ints[:, 0],\n    \"95% CI Upper\": conf_ints[:, 1]\n})\n\nprint(summary)\n\n      Parameter  Estimate  Std. Error  95% CI Lower  95% CI Upper\n0  beta_netflix  0.941195    0.100639      0.743942      1.138448\n1    beta_prime  0.501616    0.135631      0.235778      0.767453\n2      beta_ads -0.731994    0.089276     -0.906975     -0.557014\n3    beta_price -0.099480    0.006330     -0.111887     -0.087074"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\n\nX = conjoint_clean[[\"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\"]].astype(float).values\ny = conjoint_clean[\"choice\"].values\ngroup_keys = conjoint_clean[[\"resp\", \"task\"]].apply(tuple, axis=1)\nchoice_sets = [np.where(group_keys == key)[0] for key in sorted(set(group_keys))]\n\ndef log_likelihood(beta):\n    log_lik = 0.0\n    for idx in choice_sets:\n        X_set = X[idx]\n        y_set = y[idx]\n        utilities = X_set @ beta\n        log_lik += utilities[y_set == 1][0] - logsumexp(utilities)\n    return log_lik\n\n\ndef log_prior(beta):\n    prior_var = np.array([25, 25, 25, 1])  \n    return -0.5 * np.sum((beta ** 2) / prior_var)\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\nn_draws = 11000\nburn_in = 1000\nn_params = 4\nsamples = np.zeros((n_draws, n_params))\nbeta_current = np.zeros(n_params)\nlog_post_current = log_posterior(beta_current)\n\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\nfor t in range(n_draws):\n    proposal = beta_current + np.random.normal(loc=0, scale=proposal_sd)\n    log_post_proposal = log_posterior(proposal)\n    \n    log_accept_ratio = log_post_proposal - log_post_current\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    \n    samples[t] = beta_current\n\nsamples_post = samples[burn_in:]\nposterior_means = np.mean(samples_post, axis=0)\nposterior_std = np.std(samples_post, axis=0)\nposterior_ci_lower = np.percentile(samples_post, 2.5, axis=0)\nposterior_ci_upper = np.percentile(samples_post, 97.5, axis=0)\n\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nposterior_summary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Posterior Mean\": posterior_means,\n    \"Std. Dev\": posterior_std,\n    \"95% CI Lower\": posterior_ci_lower,\n    \"95% CI Upper\": posterior_ci_upper\n})\n\nprint(posterior_summary)\n\n      Parameter  Posterior Mean  Std. Dev  95% CI Lower  95% CI Upper\n0  beta_netflix        0.930949  0.108311      0.725842      1.157317\n1    beta_prime        0.488224  0.106468      0.280797      0.702842\n2      beta_ads       -0.723895  0.091051     -0.898098     -0.538874\n3    beta_price       -0.099514  0.006269     -0.112154     -0.087608\n\n\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\n\nimport matplotlib.pyplot as plt\n\nposterior_samples = samples_post  \n\nbeta_idx = 0 \nparam_name = \"Beta_Netflix\"\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(posterior_samples[:, beta_idx], color=\"blue\", alpha=0.6)\nplt.title(f\"Trace Plot: {param_name}\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\n\nplt.subplot(1, 2, 2)\nplt.hist(posterior_samples[:, beta_idx], bins=30, color=\"skyblue\", edgecolor=\"black\")\nplt.title(f\"Posterior Distribution: {param_name}\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe results from the Maximum Likelihood approach are approximately the same compared to the estimation via Bayesian Methods."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#discussion",
    "href": "blog/hw3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we did not simulate the data and only saw the parameter estimates, we would interpret the values as reflecting consumer preferences inferred from real-world consumer trends. \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) suggests that customers tend to prefer Netflix over Amazon Prime, which tracks with our intuition about streaming service brands. The higher beta parameter means that customers are more likely to choose Netflix over Prime, indicating Netflix’s higher percieved value or brand equity.\nIt makes sense that \\(\\beta_\\text{price}\\) is negative because our intuition of supply and demand is that as prices increase, utility decrease. All else equal, consumers prefer lower costs.\nFor “real world” conjoint data, we need to allow differences by individual. Therefore, we would need to allow the beta parameters to vary by individual, instead of having a fixed value across the population. Thus, the simulated data would need to have individual-level coefficients,generate choices based off of the individual beta parameters, and then estimate with a Bayesian hierarchical model.\nThis approach captures heterogeneity in preferences, which is a key aspect of real-world conjoint data, allowing different people to value different choices differently."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicole Ziola",
    "section": "",
    "text": "Current\n\nData Analyst | San Marcos Unified School District\nTeaching Assistant, Experiments in Firms | Rady School of Management\nIncoming Data & Analytics Consultant | FTI Consulting\n\n\n\nWork History\n\nEngagement Analyst Intern | The Cheesecake Factory, Inc.\nResearch Assistant | UCSD Department of Economics\n\n\n\nEducation\n\nM.S. in Business Analytics | Rady School of Management 2025\nB.S. in Business Economics | UC San Diego 2024"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download Resume"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "🎓 Education",
    "text": "🎓 Education\nMaster of Science in Business Analytics, Rady School of Management\nUniversity of California, San Diego — Expected June 2025\nGPA: 3.92/4.0\n- Relevant Coursework: Business Analytics, Collecting and Analyzing Large Data, Customer Analytics, Web Mining & Recommender Systems, Data-Driven Communications, Marketing Analytics\nBachelor of Science in Business Economics\nUniversity of California, San Diego — June 2024\nGPA: 3.93/4.0, cum laude\n- Honors: Provost’s Honors, TMC Honors Program, TRELS Scholarship Recipient\n- Relevant Coursework: Principles of Data Science, Data/Model Programming, Econometrics"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "💼 Professional Experience",
    "text": "💼 Professional Experience\nIncoming Data Analytics Consultant, FTI Consulting – Los Angeles, CA\nOct 2024 – Present\n- Leverage data science and machine learning to support forensic/litigation needs\n- Advise clients on business strategy using data-driven insights\nTeaching Assistant, Experiments in Firms – Rady School of Management\nSep 2024 – Present\n- Supported 100+ students in designing experiments and understanding causal inference\n- Taught randomization, statistical analysis, and behavioral econ principles\nEngagement Analyst Intern, The Cheesecake Factory Inc. – Calabasas, CA\nJun 2023 – Sep 2023\n- Analyzed 45,000+ survey responses, delivering insights to executives across 250+ locations\n- Designed data storytelling presentations with visualizations & correlations\n- Boosted survey engagement by 50% via policy and leadership changes\nResearch Assistant, UCSD Department of Economics – San Diego, CA\nJan 2023 – Jun 2023\n- Deployed 1,000+ surveys via Qualtrics for societal/environmental impact study\n- Used Python for data cleaning, statistical analysis, and visualization"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Resume",
    "section": "💡 Skills",
    "text": "💡 Skills\nLanguages/Tools: Python, R, SQL, ETL, Stata, PowerBI, Tableau, Glint, Qualtrics, Microsoft & Google Suite"
  },
  {
    "objectID": "resume.html#leadership-affiliations",
    "href": "resume.html#leadership-affiliations",
    "title": "Resume",
    "section": "👩‍💼 Leadership & Affiliations",
    "text": "👩‍💼 Leadership & Affiliations\nVP of Professional Development, Women in Business – San Diego, CA\nJun 2023 – Jun 2024\n- Led team of 8 to deliver a professional certification program to 250+ students\n- Increased completion rate by 40 members year-over-year"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\nNicole Ziola\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nNicole Ziola\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nNicole Ziola\nApr 18, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hw2/hw2_questions.html",
    "href": "blog/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nshow code\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nFirst, I will analyze the breakdown of patents according to customer status.\n\n\nshow code\nplt.figure(figsize=(10, 6))\n\nfor status in [0, 1]:\n    label = 'Customer' if status == 1 else 'Non-Customer'\n    subset = df[df['iscustomer'] == status]\n    plt.hist(subset['patents'], bins=20, alpha=0.5, label=label, edgecolor='black')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents by Customer Status')\nplt.legend()\nplt.show()\n\nmeans = df.groupby('iscustomer')['patents'].mean()\nmeans.index = ['Non-Customer', 'Customer']  # rename index for clarity\nprint(\"Mean number of patents by customer status:\")\nprint(means)\n\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\nNon-Customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\nThe histogram shows that both customers and non-customers have a right-skewed distribution of patent counts, with most companies holding between 2 and 4 patents. Non-customers are more frequent overall, especially at lower patent counts, though the gap narrows slightly as patent counts increase. Both groups include companies with high patent counts (10+), but these are rare. Overall, the distributions overlap substantially, suggesting customer status alone may not strongly predict the number of patents.\nNext, I will compare region and age by customer status.\n\n\nshow code\nimport seaborn as sns \n\nregion_counts = df.groupby('iscustomer')['region'].value_counts(normalize=True).unstack().fillna(0)\nprint(\"Proportion of regions by customer status:\")\nprint(region_counts)\n\nregion_counts.T.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Region Distribution by Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Status', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSummary statistics for age by customer status:\")\nprint(df.groupby('iscustomer')['age'].describe())\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='iscustomer', y='age', data=df)\nplt.xticks([0, 1], ['Non-Customer', 'Customer'])\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Customer Status')\nplt.ylabel('Age')\nplt.tight_layout()\nplt.show()\n\n\nProportion of regions by customer status:\nregion       Midwest  Northeast  Northwest     South  Southwest\niscustomer                                                     \n0           0.183513   0.267910   0.155054  0.153091   0.240432\n1           0.076923   0.681913   0.060291  0.072765   0.108108\n\n\n\n\n\n\n\n\n\n\nSummary statistics for age by customer status:\n             count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\n\n\n\n\n\n\n\nThe Northeast region stands out as having a much higher proportion of customers compared to other regions. In contrast, regions like the Midwest, Northwest, South, and Southwest have lower proportions of customers. This suggests that customers are disproportionately concentrated in the Northeast.\nThe age distributions for customers and non-customers are similar, with a slightly higher median age among customers. Both groups span a wide range of ages, but the interquartile range and spread are slightly greater for customers, indicating more age variability.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that \\(Y_1, Y_2, \\dots, Y_n\\) are i.i.d. from a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe likelihood function for \\(n\\) independent observations is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nSimplifying:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i}}{\\prod_{i=1}^{n} Y_i!}\n\\]\nNext, I will code the log-likelihood function for the Poisson model and plot it.\n\n\nshow code\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  \n\n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\n\n\n\nshow code\ndef poisson_loglikelihood(lambda_, Y):\n    if np.any(lambda_ &lt;= 0):\n        return -np.inf\n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\nY = df['patents'].values \n\nlambda_vals = np.linspace(0.1, 15, 300) \nlog_likelihoods = [poisson_loglikelihood(lam, Y) for lam in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_likelihoods, label='Log-Likelihood')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood as a Function of λ')\nplt.axvline(lambda_vals[np.argmax(log_likelihoods)], color='red', linestyle='--', label='MLE')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\), we start with the log-likelihood function for \\(n\\) observations \\(Y_1, \\dots, Y_n\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ Y_i \\log(\\lambda) - \\lambda - \\log(Y_i!) \\right]\n\\]\nWe take the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ \\frac{Y_i}{\\lambda} - 1 \\right] = \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i - n\n\\]\nSet the derivative equal to zero:\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i - n = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = n \\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\n\n\n\nThe maximum likelihood estimate of \\(\\lambda\\) is simply the sample mean, \\(\\bar{Y}\\), which aligns with intuition since the Poisson distribution has mean \\(\\lambda\\).\nUsing Python to find the MLE optimizer:\n\n\nshow code\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef neg_poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return np.inf\n    return -np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\nY = df['patents'].values  \n\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.01, 20), args=(Y,), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\n\n\nMLE for lambda: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nshow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)                \n    eta = X @ beta                         \n    eta = np.clip(eta, -20, 20)\n    lambda_ = np.exp(eta)                  \n    return -np.sum(Y * eta - lambda_ - gammaln(Y + 1)) \n\n\n\n\nshow code\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\ndf['age_sq'] = df['age'] ** 2\n\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'), \n    df[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX_mat = X.to_numpy(dtype=float)\nY = df['patents'].to_numpy(dtype=float) \n\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20) \n    lambda_ = np.exp(eta)\n    return -np.sum(Y * eta - lambda_ - gammaln(Y + 1))\n\ninit_beta = np.zeros(X_mat.shape[1])\n\nresult = minimize(poisson_loglikelihood, init_beta, args=(Y, X_mat), method='BFGS')\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nresults_df = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': std_errors\n}, index=X.columns)\n\nprint(\"\\nPoisson Regression Results:\")\nprint(results_df.round(4))\n\n\n\nPoisson Regression Results:\n            Coefficient  Std. Error\nintercept       -0.5100      0.1931\nage              0.1487      0.0145\nage_sq          -0.0030      0.0003\niscustomer       0.2076      0.0329\nNortheast        0.0292      0.0468\nNorthwest       -0.0176      0.0572\nSouth            0.0566      0.0562\nSouthwest        0.0506      0.0496\n\n\n\n\nshow code\nimport statsmodels.api as sm\n\nX_clean = X.drop(columns='intercept', errors='ignore')\n\nX_sm = sm.add_constant(X_clean)\n\nX_sm = X_sm.astype(float)\n\nY = df['patents'].astype(float).values\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresults = model.fit()\n\nclean_summary = pd.DataFrame({\n    'Coefficient': results.params,\n    'Std. Error': results.bse,\n    'Exp(Coeff)': np.exp(results.params)\n})\n\nclean_summary = clean_summary.round(4)\nclean_summary.index.name = 'Variable'\n\nprint(\"\\n📊 Poisson Regression Results:\\n\")\nprint(clean_summary.to_string())\n\n\n\n📊 Poisson Regression Results:\n\n            Coefficient  Std. Error  Exp(Coeff)\nVariable                                       \nconst           -0.5089      0.1832      0.6011\nage              0.1486      0.0139      1.1602\nage_sq          -0.0030      0.0003      0.9970\niscustomer       0.2076      0.0309      1.2307\nNortheast        0.0292      0.0436      1.0296\nNorthwest       -0.0176      0.0538      0.9826\nSouth            0.0566      0.0527      1.0582\nSouthwest        0.0506      0.0472      1.0519\n\n\n\n\n\nThe results suggest that several firm characteristics are associated with differences in the expected number of patents awarded:\n\nAge has a positive and significant effect: for each additional year, the expected number of patents increases by approximately 16% (Exp(Coeff) = 1.1602), holding other variables constant.\nAge squared has a small negative coefficient (Exp(Coeff) = 0.997), implying diminishing returns to age — the patent rate increases with age but at a decreasing rate.\nBeing a customer of Blueprinty is associated with a 23% higher patent rate (Exp(Coeff) = 1.2307) compared to non-customers, suggesting potential benefits from using the software.\nRegion effects are smaller:\n\nFirms in the South and Southwest have slightly higher expected patent counts than the base region (dropped during one-hot encoding).\nThe Northeast and Northwest show very small or negligible differences.\n\n\nTogether, the model suggests that age, customer status, and region may all influence patent outcomes, and that Blueprinty’s customers tend to have higher success rates — even after controlling for other variables.\n\n\nshow code\nX_base = X.drop(columns='intercept', errors='ignore').copy()\n\nX_base = sm.add_constant(X_base)\nX_base = X_base.astype(float)\n\nX_0 = X_base.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X_base.copy()\nX_1['iscustomer'] = 1\n\ny_pred_0 = results.predict(X_0)\ny_pred_1 = results.predict(X_1)\n\ndifference = y_pred_1 - y_pred_0\navg_diff = np.mean(difference)\n\nprint(f\"\\nAverage treatment effect of being a Blueprinty customer: {avg_diff:.4f} additional patents over 5 years.\")\n\n\n\nAverage treatment effect of being a Blueprinty customer: 0.7928 additional patents over 5 years.\n\n\nUsing Blueprinty’s software is associated with a meaningful increase in patent productivity. On average, customers produced approximately 0.79 more patents over five years than comparable non-customers. This suggests that Blueprinty contributes to greater innovation outcomes, likely by improving workflows, R&D efficiency, or idea capture."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nshow code\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nFirst, I will analyze the breakdown of patents according to customer status.\n\n\nshow code\nplt.figure(figsize=(10, 6))\n\nfor status in [0, 1]:\n    label = 'Customer' if status == 1 else 'Non-Customer'\n    subset = df[df['iscustomer'] == status]\n    plt.hist(subset['patents'], bins=20, alpha=0.5, label=label, edgecolor='black')\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Patents by Customer Status')\nplt.legend()\nplt.show()\n\nmeans = df.groupby('iscustomer')['patents'].mean()\nmeans.index = ['Non-Customer', 'Customer']  # rename index for clarity\nprint(\"Mean number of patents by customer status:\")\nprint(means)\n\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\nNon-Customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\nThe histogram shows that both customers and non-customers have a right-skewed distribution of patent counts, with most companies holding between 2 and 4 patents. Non-customers are more frequent overall, especially at lower patent counts, though the gap narrows slightly as patent counts increase. Both groups include companies with high patent counts (10+), but these are rare. Overall, the distributions overlap substantially, suggesting customer status alone may not strongly predict the number of patents.\nNext, I will compare region and age by customer status.\n\n\nshow code\nimport seaborn as sns \n\nregion_counts = df.groupby('iscustomer')['region'].value_counts(normalize=True).unstack().fillna(0)\nprint(\"Proportion of regions by customer status:\")\nprint(region_counts)\n\nregion_counts.T.plot(kind='bar', stacked=True, figsize=(10, 6))\nplt.title('Region Distribution by Customer Status')\nplt.ylabel('Proportion')\nplt.xlabel('Region')\nplt.legend(title='Customer Status', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSummary statistics for age by customer status:\")\nprint(df.groupby('iscustomer')['age'].describe())\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='iscustomer', y='age', data=df)\nplt.xticks([0, 1], ['Non-Customer', 'Customer'])\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Customer Status')\nplt.ylabel('Age')\nplt.tight_layout()\nplt.show()\n\n\nProportion of regions by customer status:\nregion       Midwest  Northeast  Northwest     South  Southwest\niscustomer                                                     \n0           0.183513   0.267910   0.155054  0.153091   0.240432\n1           0.076923   0.681913   0.060291  0.072765   0.108108\n\n\n\n\n\n\n\n\n\n\nSummary statistics for age by customer status:\n             count       mean       std   min   25%   50%    75%   max\niscustomer                                                            \n0           1019.0  26.101570  6.945426   9.0  21.0  25.5  31.25  47.5\n1            481.0  26.900208  7.814678  10.0  20.5  26.5  32.50  49.0\n\n\n\n\n\n\n\n\n\nThe Northeast region stands out as having a much higher proportion of customers compared to other regions. In contrast, regions like the Midwest, Northwest, South, and Southwest have lower proportions of customers. This suggests that customers are disproportionately concentrated in the Northeast.\nThe age distributions for customers and non-customers are similar, with a slightly higher median age among customers. Both groups span a wide range of ages, but the interquartile range and spread are slightly greater for customers, indicating more age variability.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that \\(Y_1, Y_2, \\dots, Y_n\\) are i.i.d. from a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThe likelihood function for \\(n\\) independent observations is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nSimplifying:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i}}{\\prod_{i=1}^{n} Y_i!}\n\\]\nNext, I will code the log-likelihood function for the Poisson model and plot it.\n\n\nshow code\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  \n\n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\n\n\n\nshow code\ndef poisson_loglikelihood(lambda_, Y):\n    if np.any(lambda_ &lt;= 0):\n        return -np.inf\n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\nY = df['patents'].values \n\nlambda_vals = np.linspace(0.1, 15, 300) \nlog_likelihoods = [poisson_loglikelihood(lam, Y) for lam in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, log_likelihoods, label='Log-Likelihood')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood as a Function of λ')\nplt.axvline(lambda_vals[np.argmax(log_likelihoods)], color='red', linestyle='--', label='MLE')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTo find the maximum likelihood estimator (MLE) for \\(\\lambda\\), we start with the log-likelihood function for \\(n\\) observations \\(Y_1, \\dots, Y_n\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ Y_i \\log(\\lambda) - \\lambda - \\log(Y_i!) \\right]\n\\]\nWe take the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ \\frac{Y_i}{\\lambda} - 1 \\right] = \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i - n\n\\]\nSet the derivative equal to zero:\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i - n = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = n \\quad \\Rightarrow \\quad \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\n\n\n\nThe maximum likelihood estimate of \\(\\lambda\\) is simply the sample mean, \\(\\bar{Y}\\), which aligns with intuition since the Poisson distribution has mean \\(\\lambda\\).\nUsing Python to find the MLE optimizer:\n\n\nshow code\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef neg_poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return np.inf\n    return -np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\nY = df['patents'].values  \n\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.01, 20), args=(Y,), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\n\n\nMLE for lambda: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nshow code\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)                \n    eta = X @ beta                         \n    eta = np.clip(eta, -20, 20)\n    lambda_ = np.exp(eta)                  \n    return -np.sum(Y * eta - lambda_ - gammaln(Y + 1)) \n\n\n\n\nshow code\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\ndf['age_sq'] = df['age'] ** 2\n\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'), \n    df[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX_mat = X.to_numpy(dtype=float)\nY = df['patents'].to_numpy(dtype=float) \n\ndef poisson_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    eta = X @ beta\n    eta = np.clip(eta, -20, 20) \n    lambda_ = np.exp(eta)\n    return -np.sum(Y * eta - lambda_ - gammaln(Y + 1))\n\ninit_beta = np.zeros(X_mat.shape[1])\n\nresult = minimize(poisson_loglikelihood, init_beta, args=(Y, X_mat), method='BFGS')\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nresults_df = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': std_errors\n}, index=X.columns)\n\nprint(\"\\nPoisson Regression Results:\")\nprint(results_df.round(4))\n\n\n\nPoisson Regression Results:\n            Coefficient  Std. Error\nintercept       -0.5100      0.1931\nage              0.1487      0.0145\nage_sq          -0.0030      0.0003\niscustomer       0.2076      0.0329\nNortheast        0.0292      0.0468\nNorthwest       -0.0176      0.0572\nSouth            0.0566      0.0562\nSouthwest        0.0506      0.0496\n\n\n\n\nshow code\nimport statsmodels.api as sm\n\nX_clean = X.drop(columns='intercept', errors='ignore')\n\nX_sm = sm.add_constant(X_clean)\n\nX_sm = X_sm.astype(float)\n\nY = df['patents'].astype(float).values\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresults = model.fit()\n\nclean_summary = pd.DataFrame({\n    'Coefficient': results.params,\n    'Std. Error': results.bse,\n    'Exp(Coeff)': np.exp(results.params)\n})\n\nclean_summary = clean_summary.round(4)\nclean_summary.index.name = 'Variable'\n\nprint(\"\\n📊 Poisson Regression Results:\\n\")\nprint(clean_summary.to_string())\n\n\n\n📊 Poisson Regression Results:\n\n            Coefficient  Std. Error  Exp(Coeff)\nVariable                                       \nconst           -0.5089      0.1832      0.6011\nage              0.1486      0.0139      1.1602\nage_sq          -0.0030      0.0003      0.9970\niscustomer       0.2076      0.0309      1.2307\nNortheast        0.0292      0.0436      1.0296\nNorthwest       -0.0176      0.0538      0.9826\nSouth            0.0566      0.0527      1.0582\nSouthwest        0.0506      0.0472      1.0519\n\n\n\n\n\nThe results suggest that several firm characteristics are associated with differences in the expected number of patents awarded:\n\nAge has a positive and significant effect: for each additional year, the expected number of patents increases by approximately 16% (Exp(Coeff) = 1.1602), holding other variables constant.\nAge squared has a small negative coefficient (Exp(Coeff) = 0.997), implying diminishing returns to age — the patent rate increases with age but at a decreasing rate.\nBeing a customer of Blueprinty is associated with a 23% higher patent rate (Exp(Coeff) = 1.2307) compared to non-customers, suggesting potential benefits from using the software.\nRegion effects are smaller:\n\nFirms in the South and Southwest have slightly higher expected patent counts than the base region (dropped during one-hot encoding).\nThe Northeast and Northwest show very small or negligible differences.\n\n\nTogether, the model suggests that age, customer status, and region may all influence patent outcomes, and that Blueprinty’s customers tend to have higher success rates — even after controlling for other variables.\n\n\nshow code\nX_base = X.drop(columns='intercept', errors='ignore').copy()\n\nX_base = sm.add_constant(X_base)\nX_base = X_base.astype(float)\n\nX_0 = X_base.copy()\nX_0['iscustomer'] = 0\n\nX_1 = X_base.copy()\nX_1['iscustomer'] = 1\n\ny_pred_0 = results.predict(X_0)\ny_pred_1 = results.predict(X_1)\n\ndifference = y_pred_1 - y_pred_0\navg_diff = np.mean(difference)\n\nprint(f\"\\nAverage treatment effect of being a Blueprinty customer: {avg_diff:.4f} additional patents over 5 years.\")\n\n\n\nAverage treatment effect of being a Blueprinty customer: 0.7928 additional patents over 5 years.\n\n\nUsing Blueprinty’s software is associated with a meaningful increase in patent productivity. On average, customers produced approximately 0.79 more patents over five years than comparable non-customers. This suggests that Blueprinty contributes to greater innovation outcomes, likely by improving workflows, R&D efficiency, or idea capture."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "href": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nshow code\nimport pandas as pd\ndf_airbnb = pd.read_csv('airbnb.csv')\ndf_airbnb.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt"
  },
  {
    "objectID": "blog/hw1/main.html",
    "href": "blog/hw1/main.html",
    "title": "Nicole Ziola",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nRead in data\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.columns\n\nIndex(['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25',\n       'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1',\n       'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra',\n       'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple',\n       'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse',\n       'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush',\n       'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack',\n       'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba',\n       'pop_propurban'],\n      dtype='object')\n\n\nBalance Test\n\nvariables = ['mrm2', 'hpa', 'freq']  # you can add more\n\nresults = []\n\ndef t_test_formula(x_treat, x_control):\n    # T-test using pooled standard error\n    mean1, mean2 = np.mean(x_treat), np.mean(x_control)\n    std1, std2 = np.std(x_treat, ddof=1), np.std(x_control, ddof=1)\n    n1, n2 = len(x_treat), len(x_control)\n    se = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n    t_stat = (mean1 - mean2) / se\n    return t_stat, mean1 - mean2\n\nfor var in variables:\n    treat_group = df[df['treatment'] == 1][var].dropna()\n    control_group = df[df['treatment'] == 0][var].dropna()\n\n    # --- T-test\n    t_stat, t_diff = t_test_formula(treat_group, control_group)\n\n    # --- Linear regression\n    X = sm.add_constant(df['treatment'])\n    y = df[var]\n    model = sm.OLS(y, X).fit()\n    reg_coef = model.params['treatment']\n    reg_se = model.bse['treatment']\n    reg_tstat = model.tvalues['treatment']\n\n    results.append({\n        'Variable': var,\n        'T-test statistic': round(t_stat, 4),\n        'Mean Diff (T-test)': round(t_diff, 4),\n        'Regression Coef': round(reg_coef, 4),\n        'T-stat (Regression)': round(reg_tstat, 4),\n        'Match?': 'Yes' if np.isclose(t_stat, reg_tstat, atol=1e-4) else 'No'\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n\n\n\nVariable\nT-test statistic\nMean Diff (T-test)\nRegression Coef\nT-stat (Regression)\nMatch?\n\n\n\n\n0\nmrm2\n0.1195\n0.0137\nNaN\nNaN\nNo\n\n\n1\nhpa\n0.9704\n0.6371\n0.6371\n0.9441\nNo\n\n\n2\nfreq\n-0.1108\n-0.0120\n-0.0120\n-0.1109\nYes\n\n\n\n\n\n\n\n\n# Second\n\n\n# Example proportions (replace with your actual values)\nproportion_treatment = df[df['treatment'] == 1]['gave'].mean()\nproportion_control = df[df['treatment'] == 0]['gave'].mean()\n\n# Barplot setup\nlabels = ['Control', 'Treatment']\nvalues = [proportion_control, proportion_treatment]\n\nplt.figure(figsize=(6, 4))\nplt.bar(labels, values)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rates by Group')\nplt.ylim(0, max(values)*1.2)\n\n# Add percentage labels above bars\nfor i, v in enumerate(values):\n    plt.text(i, v + 0.0005, f\"{v:.2%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# --- T-TEST: Difference in proportions for binary outcome 'gave' between treatment and control ---\n\n# Separate treatment and control groups\ngave_treatment = df[df['treatment'] == 1]['gave']\ngave_control = df[df['treatment'] == 0]['gave']\n\n# Calculate group means and sizes\np1 = gave_treatment.mean()\np2 = gave_control.mean()\nn1 = gave_treatment.shape[0]\nn2 = gave_control.shape[0]\n\n# Calculate standard error and t-statistic\nse = np.sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n2))\nt_stat = (p1 - p2) / se\n\n# Calculate degrees of freedom and p-value\ndf_ttest = n1 + n2 - 2\np_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=df_ttest))\n\nprint(\"T-test results:\")\nprint(f\"  Treatment mean: {p1:.4f}\")\nprint(f\"  Control mean: {p2:.4f}\")\nprint(f\"  T-statistic: {t_stat:.4f}\")\nprint(f\"  P-value: {p_value:.4f}\\n\")\n\n\n# --- LINEAR REGRESSION: gave ~ treatment ---\n\n# Run regression\nX = sm.add_constant(df['treatment'])\ny = df['gave']\nmodel = sm.OLS(y, X).fit()\n\nprint(\"Regression results:\")\nprint(model.summary())\n\nT-test results:\n  Treatment mean: 0.0220\n  Control mean: 0.0179\n  T-statistic: 3.2095\n  P-value: 0.0013\n\nRegression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        15:12:52   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport statsmodels.formula.api as smf\n\n# Probit regression of 'gave' on 'treatment'\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 21 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:12:53   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Subset groups based on match ratio\ngave_ratio1 = df[df['ratio'] == 1]['gave']\ngave_ratio2 = df[df['ratio'] == 2]['gave']\ngave_ratio3 = df[df['ratio'] == 3]['gave']\n\n# Perform independent t-tests\nt_2v1, p_2v1 = ttest_ind(gave_ratio2, gave_ratio1, equal_var=False)\nt_3v2, p_3v2 = ttest_ind(gave_ratio3, gave_ratio2, equal_var=False)\nt_3v1, p_3v1 = ttest_ind(gave_ratio3, gave_ratio1, equal_var=False)\n\n# Print results\nprint(\"T-Test Results (Donation Likelihood by Match Ratio):\\n\")\nprint(f\"2:1 vs 1:1 -&gt; t = {t_2v1:.3f}, p = {p_2v1:.4f}\")\nprint(f\"3:1 vs 2:1 -&gt; t = {t_3v2:.3f}, p = {p_3v2:.4f}\")\nprint(f\"3:1 vs 1:1 -&gt; t = {t_3v1:.3f}, p = {p_3v1:.4f}\")\n\nT-Test Results (Donation Likelihood by Match Ratio):\n\n2:1 vs 1:1 -&gt; t = 0.965, p = 0.3345\n3:1 vs 2:1 -&gt; t = 0.050, p = 0.9600\n3:1 vs 1:1 -&gt; t = 1.015, p = 0.3101\n\n\n\n# Regress using 'ratio' as a categorical variable (automatically handles dummies)\ncat_model = smf.ols('gave ~ C(ratio)', data=df).fit()\n\nprint(\"\\nRegression with Categorical Variable:\")\nprint(cat_model.summary())\n\n\nRegression with Categorical Variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0118\nTime:                        15:33:53   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0179      0.001     16.225      0.000       0.016       0.020\nC(ratio)[T.1]     0.0029      0.002      1.661      0.097      -0.001       0.006\nC(ratio)[T.2]     0.0048      0.002      2.744      0.006       0.001       0.008\nC(ratio)[T.3]     0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport statsmodels.formula.api as smf\n\n# --- RAW RESPONSE RATE DIFFERENCES ---\n# Compute average donation rates by ratio level\nresponse_rate_1to1 = df[df['ratio'] == 1]['gave'].mean()\nresponse_rate_2to1 = df[df['ratio'] == 2]['gave'].mean()\nresponse_rate_3to1 = df[df['ratio'] == 3]['gave'].mean()\n\n# Calculate raw differences\nraw_diff_2v1 = response_rate_2to1 - response_rate_1to1\nraw_diff_3v2 = response_rate_3to1 - response_rate_2to1\n\nprint(\"Raw response rate differences:\")\nprint(f\"  2:1 vs 1:1: {raw_diff_2v1:.4f}\")\nprint(f\"  3:1 vs 2:1: {raw_diff_3v2:.4f}\\n\")\n\n# --- REGRESSION APPROACH ---\n# Create dummy variables if not already present\ndf['ratio2'] = (df['ratio'] == 2).astype(int)\ndf['ratio3'] = (df['ratio'] == 3).astype(int)\n\n# Fit model using 1:1 as baseline\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=df).fit()\n\n# Extract regression-based differences\nreg_diff_2v1 = model.params['ratio2']\nreg_diff_3v2 = model.params['ratio3'] - model.params['ratio2']\n\nprint(\"Regression-estimated response rate differences:\")\nprint(f\"  2:1 vs 1:1: {reg_diff_2v1:.4f}\")\nprint(f\"  3:1 vs 2:1: {reg_diff_3v2:.4f}\")\n\nRaw response rate differences:\n  2:1 vs 1:1: 0.0019\n  3:1 vs 2:1: 0.0001\n\nRegression-estimated response rate differences:\n  2:1 vs 1:1: 0.0036\n  3:1 vs 2:1: 0.0001\n\n\n\n### Size of Charitable Contribution\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# --- T-TEST ---\n# Compare donation amounts between treatment and control groups\namount_treatment = df[df['treatment'] == 1]['amount']\namount_control = df[df['treatment'] == 0]['amount']\n\n# T-test (unequal variance)\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\nprint(\"T-test: Donation Amount ~ Treatment\")\nprint(f\"  T-statistic: {t_stat:.3f}\")\nprint(f\"  P-value: {p_val:.4f}\\n\")\n\n# --- REGRESSION ---\n# Run bivariate regression\nmodel = smf.ols('amount ~ treatment', data=df).fit()\n\nprint(\"Bivariate Linear Regression: Donation Amount ~ Treatment\")\nprint(model.summary())\n\nT-test: Donation Amount ~ Treatment\n  T-statistic: 1.918\n  P-value: 0.0551\n\nBivariate Linear Regression: Donation Amount ~ Treatment\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        15:37:20   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# --- FILTER: Only people who donated ---\ndf_donors = df[df['gave'] == 1]\n\n# --- T-TEST on Amount Given ---\namount_treatment = df_donors[df_donors['treatment'] == 1]['amount']\namount_control = df_donors[df_donors['treatment'] == 0]['amount']\n\nt_stat, p_val = ttest_ind(amount_treatment, amount_control, equal_var=False)\n\nprint(\"T-test (Amount Given | Gave = 1):\")\nprint(f\"  T-statistic: {t_stat:.3f}\")\nprint(f\"  P-value: {p_val:.4f}\\n\")\n\n# --- REGRESSION: Donation Amount ~ Treatment (only donors) ---\nmodel = smf.ols('amount ~ treatment', data=df_donors).fit()\n\nprint(\"OLS Regression (Amount Given | Gave = 1):\")\nprint(model.summary())\n\nT-test (Amount Given | Gave = 1):\n  T-statistic: -0.585\n  P-value: 0.5590\n\nOLS Regression (Amount Given | Gave = 1):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.561\nTime:                        15:37:43   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to only people who donated\ndf_donors = df[df['gave'] == 1]\n\n# Split into treatment and control\ntreatment_donors = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_donors = df_donors[df_donors['treatment'] == 0]['amount']\n\n# Calculate means\nmean_treatment = treatment_donors.mean()\nmean_control = control_donors.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_control:.2f}')\naxes[0].set_title(\"Control Group (Donors Only)\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treatment_donors, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_treatment:.2f}')\naxes[1].set_title(\"Treatment Group (Donors Only)\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter to only people who donated\ndf_donors = df[df['gave'] == 1]\n\n# Control and treatment donation amounts\ncontrol = df_donors[df_donors['treatment'] == 0]['amount'].values\ntreatment = df_donors[df_donors['treatment'] == 1]['amount'].values\n\n# True difference in means\ntrue_diff = treatment.mean() - control.mean()\n\n# Simulate draws\nnp.random.seed(42)\ncontrol_draws = np.random.choice(control, size=100000, replace=True)\ntreatment_draws = np.random.choice(treatment, size=10000, replace=True)\n\n# Sample 10,000 control values from the 100,000 to match length\ncontrol_draws_subset = np.random.choice(control_draws, size=10000, replace=False)\n\n# Calculate vector of differences\ndifferences = treatment_draws - control_draws_subset\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.2f}')\nplt.title(\"Cumulative Average of Simulated Differences in Donation Amounts\")\nplt.xlabel(\"Number of Simulated Differences\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Filter to only people who donated\ndf_donors = df[df['gave'] == 1]\n\n# Extract treatment and control donation values\ncontrol = df_donors[df_donors['treatment'] == 0]['amount'].values\ntreatment = df_donors[df_donors['treatment'] == 1]['amount'].values\n\n# Sample sizes and number of simulations\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Setup 2x2 plot\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nnp.random.seed(42)\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n    for _ in range(num_simulations):\n        control_sample = np.random.choice(control, size=n, replace=True)\n        treatment_sample = np.random.choice(treatment, size=n, replace=True)\n        avg_diffs.append(np.mean(treatment_sample) - np.mean(control_sample))\n    \n    ax = axes[i]\n    ax.hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    ax.axvline(0, color='red', linestyle='--', label='Zero')\n    ax.set_title(f\"Sample Size = {n}\")\n    ax.set_xlabel(\"Avg Difference (Treatment - Control)\")\n    ax.set_ylabel(\"Frequency\")\n    ax.legend()\n\nplt.suptitle(\"Sampling Distribution of Mean Differences (Treatment - Control)\", fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()"
  }
]