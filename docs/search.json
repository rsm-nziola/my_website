[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\nNicole Ziola\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nNicole Ziola\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nNicole Ziola\nApr 18, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html",
    "href": "blog/hw3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/hw3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\nshow code\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n\nbrand = [\"N\", \"P\", \"H\"] \nad = [\"Yes\", \"No\"]\nprice = np.arange(8, 33, 4)\n\nimport itertools\nprofiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=[\"brand\", \"ad\", \"price\"])\nm = len(profiles)\n\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\ndef sim_one(id):\n    datlist = []\n\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled.insert(0, \"resp\", id)\n        sampled.insert(1, \"task\", t)\n\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util) +\n            sampled[\"ad\"].map(a_util) +\n            sampled[\"price\"].apply(p_util)\n        ).round(10)\n\n        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        datlist.append(sampled)\n\n    return pd.concat(datlist, ignore_index=True)\n\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\nprint(conjoint_data.head())\n\n\n   resp  task brand  ad  price  choice\n0     1     1     P  No     32       0\n1     1     1     N  No     28       0\n2     1     1     N  No     24       1\n3     1     2     H  No     28       0\n4     1     2     H  No      8       1"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nshow code\nimport pandas as pd\n\ndf = pd.read_csv('conjoint_data.csv')\n\nconjoint_clean = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\nconjoint_clean = conjoint_clean.rename(columns={\n    \"brand_N\": \"brand_netflix\",\n    \"brand_P\": \"brand_prime\",\n    \"ad_Yes\": \"ad_yes\"\n})\n\nconjoint_clean[\"alt_id\"] = conjoint_clean.groupby([\"resp\", \"task\"]).cumcount() + 1\n\nconjoint_clean = conjoint_clean[[\n    \"resp\", \"task\", \"alt_id\",\n    \"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\",\n    \"choice\"\n]]\n\n\nprint(conjoint_clean.head(10))\n\n\n   resp  task  alt_id  brand_netflix  brand_prime  ad_yes  price  choice\n0     1     1       1           True        False    True     28       1\n1     1     1       2          False        False    True     16       0\n2     1     1       3          False         True    True     16       0\n3     1     2       1           True        False    True     32       0\n4     1     2       2          False         True    True     16       1\n5     1     2       3           True        False    True     24       0\n6     1     3       1          False         True   False      8       0\n7     1     3       2          False        False    True     24       1\n8     1     3       3           True        False   False     24       0\n9     1     4       1          False         True   False     28       0"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nIn this section, we estimate the part-worth utilities (preference weights) for each attribute level using Maximum Likelihood Estimation (MLE). We assume a Multinomial Logit (MNL) model, and use the cleaned dataset to compute parameter estimates, standard errors, and 95% confidence intervals. These estimates reflect how much each attribute (e.g., Netflix brand, ads, price) influences consumer choices.\n\n\nshow code\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\n\nX = conjoint_clean[[\"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\"]].astype(float).values\ny = conjoint_clean[\"choice\"].values\n\ngroups = conjoint_clean.groupby([\"resp\", \"task\"]).indices\nchoice_sets = list(groups.values())\n\ndef log_likelihood(beta):\n    log_lik = 0\n    for idx in choice_sets:\n        X_set = X[idx]\n        y_set = y[idx]\n        utilities = X_set @ beta\n        exp_utilities = np.exp(utilities)\n        probs = exp_utilities / np.sum(exp_utilities)\n        log_lik += np.log(probs[y_set == 1][0])\n    return -log_lik  \n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(log_likelihood, initial_beta, method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nz = 1.96  # critical value for 95% CI\nconf_ints = np.vstack([beta_hat - z * std_errors, beta_hat + z * std_errors]).T\n\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nsummary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": std_errors,\n    \"95% CI Lower\": conf_ints[:, 0],\n    \"95% CI Upper\": conf_ints[:, 1]\n})\n\nprint(summary)\n\n\n      Parameter  Estimate  Std. Error  95% CI Lower  95% CI Upper\n0  beta_netflix  0.941195    0.100639      0.743942      1.138448\n1    beta_prime  0.501616    0.135631      0.235778      0.767453\n2      beta_ads -0.731994    0.089276     -0.906975     -0.557014\n3    beta_price -0.099480    0.006330     -0.111887     -0.087074"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nHere we estimate the same part-worth utilities using a Bayesian framework. Instead of point estimates, we generate posterior distributions for each parameter using a sampling method (e.g., MCMC). This approach provides a fuller view of uncertainty and allows us to visualize the posterior mean, standard deviation, and credible intervals. Trace plots and posterior densities help diagnose convergence and distribution shape.\n\n\nshow code\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\n\nX = conjoint_clean[[\"brand_netflix\", \"brand_prime\", \"ad_yes\", \"price\"]].astype(float).values\ny = conjoint_clean[\"choice\"].values\ngroup_keys = conjoint_clean[[\"resp\", \"task\"]].apply(tuple, axis=1)\nchoice_sets = [np.where(group_keys == key)[0] for key in sorted(set(group_keys))]\n\ndef log_likelihood(beta):\n    log_lik = 0.0\n    for idx in choice_sets:\n        X_set = X[idx]\n        y_set = y[idx]\n        utilities = X_set @ beta\n        log_lik += utilities[y_set == 1][0] - logsumexp(utilities)\n    return log_lik\n\n\ndef log_prior(beta):\n    prior_var = np.array([25, 25, 25, 1])  \n    return -0.5 * np.sum((beta ** 2) / prior_var)\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\nn_draws = 11000\nburn_in = 1000\nn_params = 4\nsamples = np.zeros((n_draws, n_params))\nbeta_current = np.zeros(n_params)\nlog_post_current = log_posterior(beta_current)\n\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\nfor t in range(n_draws):\n    proposal = beta_current + np.random.normal(loc=0, scale=proposal_sd)\n    log_post_proposal = log_posterior(proposal)\n    \n    log_accept_ratio = log_post_proposal - log_post_current\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    \n    samples[t] = beta_current\n\nsamples_post = samples[burn_in:]\nposterior_means = np.mean(samples_post, axis=0)\nposterior_std = np.std(samples_post, axis=0)\nposterior_ci_lower = np.percentile(samples_post, 2.5, axis=0)\nposterior_ci_upper = np.percentile(samples_post, 97.5, axis=0)\n\nparam_names = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nposterior_summary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Posterior Mean\": posterior_means,\n    \"Std. Dev\": posterior_std,\n    \"95% CI Lower\": posterior_ci_lower,\n    \"95% CI Upper\": posterior_ci_upper\n})\n\nprint(posterior_summary)\n\n\n      Parameter  Posterior Mean  Std. Dev  95% CI Lower  95% CI Upper\n0  beta_netflix        0.930949  0.108311      0.725842      1.157317\n1    beta_prime        0.488224  0.106468      0.280797      0.702842\n2      beta_ads       -0.723895  0.091051     -0.898098     -0.538874\n3    beta_price       -0.099514  0.006269     -0.112154     -0.087608\n\n\n\n\nshow code\nimport matplotlib.pyplot as plt\n\nposterior_samples = samples_post  \n\nbeta_idx = 0 \nparam_name = \"Beta_Netflix\"\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(posterior_samples[:, beta_idx], color=\"blue\", alpha=0.6)\nplt.title(f\"Trace Plot: {param_name}\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\n\nplt.subplot(1, 2, 2)\nplt.hist(posterior_samples[:, beta_idx], bins=30, color=\"skyblue\", edgecolor=\"black\")\nplt.title(f\"Posterior Distribution: {param_name}\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe results from the Maximum Likelihood approach are approximately the same compared to the estimation via Bayesian Methods. This is expected because of the large dataset and similar specifications amongst both models."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#discussion",
    "href": "blog/hw3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we did not simulate the data and only saw the parameter estimates, we would interpret the values as reflecting consumer preferences inferred from real-world consumer trends. \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) suggests that customers tend to prefer Netflix over Amazon Prime, which tracks with our intuition about streaming service brands. The higher beta parameter means that customers are more likely to choose Netflix over Prime, indicating Netflix’s higher percieved value or brand equity.\nIt makes sense that \\(\\beta_\\text{price}\\) is negative because our intuition of supply and demand is that as prices increase, utility decrease. All else equal, consumers prefer lower costs.\nFor “real world” conjoint data, we need to allow differences by individual. Therefore, we would need to allow the beta parameters to vary by individual, instead of having a fixed value across the population. Thus, the simulated data would need to have individual-level coefficients,generate choices based off of the individual beta parameters, and then estimate with a Bayesian hierarchical model.\nThis approach captures heterogeneity in preferences, which is a key aspect of real-world conjoint data, allowing different people to value different choices differently."
  }
]